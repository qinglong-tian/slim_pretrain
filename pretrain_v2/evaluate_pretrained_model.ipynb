{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of `pretrain_v2` Pretrained Model\n",
        "\n",
        "This notebook evaluates `pretrain_v2/pretrained_model/latest.pt` on a small set of binary classification benchmarks.\n",
        "\n",
        "- Includes dataset types: mostly/all continuous, mixed continuous+categorical, mostly/all categorical.\n",
        "- Builds feature metadata required by the model:\n",
        "  - `feature_is_categorical`\n",
        "  - `feature_cardinalities`\n",
        "- Converts each dataset to PU format for 10 independent replicates.\n",
        "- Reports average classification + outlier-detection metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "ba63ff48",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from io import BytesIO, TextIOWrapper\n",
        "import math\n",
        "import sys\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from scipy.io import arff\n",
        "from urllib.request import urlretrieve\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    balanced_accuracy_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "7a57a9c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repo_root=/Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/slim_pretrain\n",
            "pretrain_root=/Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/slim_pretrain/pretrain_v2\n"
          ]
        }
      ],
      "source": [
        "# Path resolution for both full-repo and standalone pretrain_v2 usage.\n",
        "cwd = Path.cwd().resolve()\n",
        "if (cwd / \"pretrain_v2\").exists():\n",
        "    repo_root = cwd\n",
        "    pretrain_root = cwd / \"pretrain_v2\"\n",
        "elif (cwd / \"model.py\").exists() and (cwd / \"__init__.py\").exists() and cwd.name == \"pretrain_v2\":\n",
        "    pretrain_root = cwd\n",
        "    repo_root = cwd.parent\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        \"Run this notebook either from the repo root (containing pretrain_v2/) \"\n",
        "        \"or from inside the pretrain_v2 folder.\"\n",
        "    )\n",
        "\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "from pretrain_v2.model import NanoTabPFNPUModel\n",
        "\n",
        "print(f\"repo_root={repo_root}\")\n",
        "print(f\"pretrain_root={pretrain_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "8a44bafa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE=mps\n",
            "CHECKPOINT_PATH=/Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/slim_pretrain/pretrain_v2/pretrained_model/latest.pt\n",
            "LEGACY_CHECKPOINT_PATH=/Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/slim_pretrain/pretrain_v2/saved_models/legacy_model.pt\n",
            "LEGACY_MODEL_COMMIT=bfa65b8\n"
          ]
        }
      ],
      "source": [
        "# ===== User-configurable evaluation settings =====\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    DEVICE = \"mps\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "CHECKPOINT_PATH = pretrain_root / \"pretrained_model\" / \"latest.pt\"\n",
        "LEGACY_CHECKPOINT_PATH = pretrain_root / \"saved_models\" / \"legacy_model.pt\"\n",
        "LEGACY_MODEL_COMMIT = \"bfa65b8\"\n",
        "\n",
        "OUTPUT_DIR = pretrain_root / \"evaluation_outputs\"\n",
        "CACHE_DIR = pretrain_root / \".cache\"\n",
        "\n",
        "# Download UCI Repository datasets (required for this benchmark set).\n",
        "ALLOW_UCI_DOWNLOAD = True\n",
        "\n",
        "N_REPLICATES = 10\n",
        "MAX_ATTEMPTS_PER_DATASET = 200\n",
        "\n",
        "MAX_POSITIVE_SIZE = 900\n",
        "UNLABELED_LABELED_POSITIVE_RATIO = (1, 4)  # unlabeled:labeled among selected positives\n",
        "OUTLIER_RATE = 0.5  # fraction of outliers in unlabeled set\n",
        "\n",
        "GLOBAL_SEED = 42\n",
        "\n",
        "if not CHECKPOINT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "if not LEGACY_CHECKPOINT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Legacy checkpoint not found: {LEGACY_CHECKPOINT_PATH}\")\n",
        "if not (0.0 <= OUTLIER_RATE < 1.0):\n",
        "    raise ValueError(\"OUTLIER_RATE must satisfy 0 <= OUTLIER_RATE < 1.\")\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"DEVICE={DEVICE}\")\n",
        "print(f\"CHECKPOINT_PATH={CHECKPOINT_PATH}\")\n",
        "print(f\"LEGACY_CHECKPOINT_PATH={LEGACY_CHECKPOINT_PATH}\")\n",
        "print(f\"LEGACY_MODEL_COMMIT={LEGACY_MODEL_COMMIT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "7c1d3d41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded models for comparison.\n",
            "max_categorical_classes=64\n",
            "latest model: missing_keys=0, unexpected_keys=0\n",
            "legacy model: missing_keys=0, unexpected_keys=0\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.modules.transformer import LayerNorm, Linear, MultiheadAttention\n",
        "\n",
        "\n",
        "class LegacyNanoTabPFNPUModel(nn.Module):\n",
        "    \"\"\"Legacy PU-adapted NanoTabPFN model from commit bfa65b8.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size: int,\n",
        "        num_attention_heads: int,\n",
        "        mlp_hidden_size: int,\n",
        "        num_layers: int,\n",
        "        num_outputs: int = 2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.feature_encoder = LegacyFeatureEncoder(embedding_size)\n",
        "        self.target_encoder = LegacyTargetEncoderPU(embedding_size)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [\n",
        "                LegacyTransformerEncoderLayerPU(\n",
        "                    embedding_size=embedding_size,\n",
        "                    nhead=num_attention_heads,\n",
        "                    mlp_hidden_size=mlp_hidden_size,\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.decoder = LegacyDecoder(embedding_size, mlp_hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, src: tuple[torch.Tensor, torch.Tensor], train_test_split_index: int) -> torch.Tensor:\n",
        "        x_src, y_src = src\n",
        "        if len(y_src.shape) < len(x_src.shape):\n",
        "            y_src = y_src.unsqueeze(-1)\n",
        "\n",
        "        x_src = self.feature_encoder(x_src, train_test_split_index)\n",
        "        num_rows = x_src.shape[1]\n",
        "        y_src = self.target_encoder(y_src, num_rows)\n",
        "        src_table = torch.cat([x_src, y_src], dim=2)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            src_table = block(src_table, train_test_split_index=train_test_split_index)\n",
        "\n",
        "        output = src_table[:, train_test_split_index:, -1, :]\n",
        "        return self.decoder(output)\n",
        "\n",
        "\n",
        "class LegacyFeatureEncoder(nn.Module):\n",
        "    def __init__(self, embedding_size: int):\n",
        "        super().__init__()\n",
        "        self.linear_layer = nn.Linear(1, embedding_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, train_test_split_index: int) -> torch.Tensor:\n",
        "        x = x.unsqueeze(-1)\n",
        "        train_rows = int(max(0, min(train_test_split_index, x.shape[1])))\n",
        "        if train_rows >= 2:\n",
        "            train_slice = x[:, :train_rows]\n",
        "            mean = torch.mean(train_slice, dim=1, keepdim=True)\n",
        "            std = torch.std(train_slice, dim=1, keepdim=True, unbiased=False).clamp_min(1e-20)\n",
        "        elif train_rows == 1:\n",
        "            train_slice = x[:, :1]\n",
        "            mean = torch.mean(train_slice, dim=1, keepdim=True)\n",
        "            std = torch.ones_like(mean)\n",
        "        else:\n",
        "            mean = torch.zeros_like(x[:, :1])\n",
        "            std = torch.ones_like(x[:, :1])\n",
        "        x = (x - mean) / std\n",
        "        x = torch.clip(x, min=-100, max=100)\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class LegacyTargetEncoderPU(nn.Module):\n",
        "    def __init__(self, embedding_size: int):\n",
        "        super().__init__()\n",
        "        self.linear_layer = nn.Linear(1, embedding_size)\n",
        "        self.unlabeled_embedding = nn.Parameter(torch.zeros(1, 1, embedding_size))\n",
        "        nn.init.normal_(self.unlabeled_embedding, std=0.02)\n",
        "\n",
        "    def forward(self, y_train: torch.Tensor, num_rows: int) -> torch.Tensor:\n",
        "        if y_train.dim() == 2:\n",
        "            y_train = y_train.unsqueeze(-1)\n",
        "        if y_train.shape[1] > num_rows:\n",
        "            raise ValueError(\"y_train rows exceed total num_rows.\")\n",
        "\n",
        "        batch_size = y_train.shape[0]\n",
        "        pad_rows = num_rows - y_train.shape[1]\n",
        "        if pad_rows > 0:\n",
        "            padding = torch.full(\n",
        "                (batch_size, pad_rows, 1),\n",
        "                -1.0,\n",
        "                dtype=y_train.dtype,\n",
        "                device=y_train.device,\n",
        "            )\n",
        "            y_full = torch.cat([y_train, padding], dim=1)\n",
        "        else:\n",
        "            y_full = y_train\n",
        "\n",
        "        observed_mask = y_full >= 0\n",
        "        y_for_linear = y_full.clone()\n",
        "        y_for_linear[~observed_mask] = 0.0\n",
        "        embedded = self.linear_layer(y_for_linear)\n",
        "\n",
        "        unlabeled = self.unlabeled_embedding.expand(batch_size, num_rows, -1)\n",
        "        embedded = torch.where(observed_mask.expand_as(embedded), embedded, unlabeled)\n",
        "        return embedded.unsqueeze(2)\n",
        "\n",
        "\n",
        "class LegacyTransformerEncoderLayerPU(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size: int,\n",
        "        nhead: int,\n",
        "        mlp_hidden_size: int,\n",
        "        layer_norm_eps: float = 1e-5,\n",
        "        batch_first: bool = True,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.self_attention_between_datapoints = MultiheadAttention(\n",
        "            embedding_size, nhead, batch_first=batch_first, device=device, dtype=dtype\n",
        "        )\n",
        "        self.self_attention_between_features = MultiheadAttention(\n",
        "            embedding_size, nhead, batch_first=batch_first, device=device, dtype=dtype\n",
        "        )\n",
        "\n",
        "        self.linear1 = Linear(embedding_size, mlp_hidden_size, device=device, dtype=dtype)\n",
        "        self.linear2 = Linear(mlp_hidden_size, embedding_size, device=device, dtype=dtype)\n",
        "\n",
        "        self.norm1 = LayerNorm(embedding_size, eps=layer_norm_eps, device=device, dtype=dtype)\n",
        "        self.norm2 = LayerNorm(embedding_size, eps=layer_norm_eps, device=device, dtype=dtype)\n",
        "        self.norm3 = LayerNorm(embedding_size, eps=layer_norm_eps, device=device, dtype=dtype)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, train_test_split_index: int) -> torch.Tensor:\n",
        "        batch_size, rows_size, col_size, embedding_size = src.shape\n",
        "\n",
        "        src_f = src.reshape(batch_size * rows_size, col_size, embedding_size)\n",
        "        src_f = self.self_attention_between_features(src_f, src_f, src_f)[0] + src_f\n",
        "        src = src_f.reshape(batch_size, rows_size, col_size, embedding_size)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        src = src.transpose(1, 2)\n",
        "        src_d = src.reshape(batch_size * col_size, rows_size, embedding_size)\n",
        "\n",
        "        src_left = self.self_attention_between_datapoints(\n",
        "            src_d[:, :train_test_split_index],\n",
        "            src_d[:, :train_test_split_index],\n",
        "            src_d[:, :train_test_split_index],\n",
        "        )[0]\n",
        "        src_right = self.self_attention_between_datapoints(\n",
        "            src_d[:, train_test_split_index:],\n",
        "            src_d,\n",
        "            src_d,\n",
        "        )[0]\n",
        "        src_d = torch.cat([src_left, src_right], dim=1) + src_d\n",
        "\n",
        "        src = src_d.reshape(batch_size, col_size, rows_size, embedding_size)\n",
        "        src = src.transpose(2, 1)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        src = self.linear2(F.gelu(self.linear1(src))) + src\n",
        "        src = self.norm3(src)\n",
        "        return src\n",
        "\n",
        "\n",
        "class LegacyDecoder(nn.Module):\n",
        "    def __init__(self, embedding_size: int, mlp_hidden_size: int, num_outputs: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embedding_size, mlp_hidden_size)\n",
        "        self.linear2 = nn.Linear(mlp_hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear2(F.gelu(self.linear1(x)))\n",
        "\n",
        "\n",
        "def load_latest_model_from_checkpoint(checkpoint_path: Path, device: str = \"cpu\"):\n",
        "    payload = torch.load(checkpoint_path, map_location=device)\n",
        "    model_cfg = payload.get(\"config\", {}).get(\"model\", {})\n",
        "\n",
        "    model = NanoTabPFNPUModel(\n",
        "        embedding_size=int(model_cfg.get(\"embedding_size\", 128)),\n",
        "        num_attention_heads=int(model_cfg.get(\"num_attention_heads\", 8)),\n",
        "        mlp_hidden_size=int(model_cfg.get(\"mlp_hidden_size\", 256)),\n",
        "        num_layers=int(model_cfg.get(\"num_layers\", 6)),\n",
        "        num_outputs=int(model_cfg.get(\"num_outputs\", 2)),\n",
        "        max_categorical_classes=int(model_cfg.get(\"max_categorical_classes\", 64)),\n",
        "    ).to(device)\n",
        "\n",
        "    state_dict = payload.get(\"model_state_dict\", payload)\n",
        "    load_result = model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model, payload, load_result\n",
        "\n",
        "\n",
        "def load_legacy_model_from_checkpoint(checkpoint_path: Path, device: str = \"cpu\"):\n",
        "    payload = torch.load(checkpoint_path, map_location=device)\n",
        "    model_cfg = payload.get(\"config\", {}).get(\"model\", {})\n",
        "\n",
        "    model = LegacyNanoTabPFNPUModel(\n",
        "        embedding_size=int(model_cfg.get(\"embedding_size\", 128)),\n",
        "        num_attention_heads=int(model_cfg.get(\"num_attention_heads\", 8)),\n",
        "        mlp_hidden_size=int(model_cfg.get(\"mlp_hidden_size\", 256)),\n",
        "        num_layers=int(model_cfg.get(\"num_layers\", 6)),\n",
        "        num_outputs=int(model_cfg.get(\"num_outputs\", 2)),\n",
        "    ).to(device)\n",
        "\n",
        "    state_dict = payload.get(\"model_state_dict\", payload)\n",
        "    load_result = model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model, payload, load_result\n",
        "\n",
        "\n",
        "latest_model, latest_payload, latest_load_result = load_latest_model_from_checkpoint(CHECKPOINT_PATH, device=DEVICE)\n",
        "legacy_model, legacy_payload, legacy_load_result = load_legacy_model_from_checkpoint(LEGACY_CHECKPOINT_PATH, device=DEVICE)\n",
        "\n",
        "MAX_CATEGORICAL_CLASSES = int(latest_model.feature_encoder.categorical_embedding.num_embeddings - 1)\n",
        "\n",
        "MODEL_SPECS = [\n",
        "    {\n",
        "        \"model_name\": \"latest\",\n",
        "        \"model\": latest_model,\n",
        "        \"supports_categorical\": True,\n",
        "        \"checkpoint_path\": str(CHECKPOINT_PATH),\n",
        "    },\n",
        "    {\n",
        "        \"model_name\": \"legacy\",\n",
        "        \"model\": legacy_model,\n",
        "        \"supports_categorical\": False,\n",
        "        \"checkpoint_path\": str(LEGACY_CHECKPOINT_PATH),\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"Loaded models for comparison.\")\n",
        "print(f\"max_categorical_classes={MAX_CATEGORICAL_CLASSES}\")\n",
        "print(\n",
        "    \"latest model: \"\n",
        "    f\"missing_keys={len(latest_load_result.missing_keys)}, unexpected_keys={len(latest_load_result.unexpected_keys)}\"\n",
        ")\n",
        "print(\n",
        "    \"legacy model: \"\n",
        "    f\"missing_keys={len(legacy_load_result.missing_keys)}, unexpected_keys={len(legacy_load_result.unexpected_keys)}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "76697466",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>source</th>\n",
              "      <th>rows</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uci_wdbc_continuous</td>\n",
              "      <td>uci:wdbc</td>\n",
              "      <td>569</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uci_adult_mixed</td>\n",
              "      <td>uci:adult</td>\n",
              "      <td>30162</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uci_spambase_continuous</td>\n",
              "      <td>uci:spambase</td>\n",
              "      <td>4601</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uci_mushroom_categorical</td>\n",
              "      <td>uci:mushroom</td>\n",
              "      <td>8124</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uci_magic_gamma_continuous</td>\n",
              "      <td>uci:magic-gamma-telescope</td>\n",
              "      <td>19020</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>uci_car_evaluation_categorical</td>\n",
              "      <td>uci:car-evaluation</td>\n",
              "      <td>1728</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>uci_banknote_authentication_continuous</td>\n",
              "      <td>uci:banknote-authentication</td>\n",
              "      <td>1372</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>uci_rice_cammeo_osmancik_continuous</td>\n",
              "      <td>uci:rice-cammeo-and-osmancik</td>\n",
              "      <td>3810</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  dataset                        source  \\\n",
              "0                     uci_wdbc_continuous                      uci:wdbc   \n",
              "1                         uci_adult_mixed                     uci:adult   \n",
              "2                 uci_spambase_continuous                  uci:spambase   \n",
              "3                uci_mushroom_categorical                  uci:mushroom   \n",
              "4              uci_magic_gamma_continuous     uci:magic-gamma-telescope   \n",
              "5          uci_car_evaluation_categorical            uci:car-evaluation   \n",
              "6  uci_banknote_authentication_continuous   uci:banknote-authentication   \n",
              "7     uci_rice_cammeo_osmancik_continuous  uci:rice-cammeo-and-osmancik   \n",
              "\n",
              "    rows  features  \n",
              "0    569        30  \n",
              "1  30162        14  \n",
              "2   4601        57  \n",
              "3   8124        21  \n",
              "4  19020        10  \n",
              "5   1728         6  \n",
              "6   1372         4  \n",
              "7   3810         7  "
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UCI_BASE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
        "\n",
        "\n",
        "def _download_with_cache(url: str, subdir: str, filename: str, allow_download: bool = True) -> Path:\n",
        "    target_dir = CACHE_DIR / \"uci\" / subdir\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    local_path = target_dir / filename\n",
        "\n",
        "    if local_path.exists():\n",
        "        return local_path\n",
        "    if not allow_download:\n",
        "        raise FileNotFoundError(\n",
        "            f\"UCI cached file not found and downloads are disabled: {local_path}. \"\n",
        "            \"Set ALLOW_UCI_DOWNLOAD=True to fetch it.\"\n",
        "        )\n",
        "\n",
        "    urlretrieve(url, local_path)\n",
        "    return local_path\n",
        "\n",
        "\n",
        "def _strip_object_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    object_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns\n",
        "    for col in object_cols:\n",
        "        df[col] = df[col].astype(\"string\").str.strip()\n",
        "    return df\n",
        "\n",
        "\n",
        "def _read_uci_table_from_zip(zip_path: Path) -> pd.DataFrame:\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        members = [\n",
        "            name\n",
        "            for name in zf.namelist()\n",
        "            if not name.endswith(\"/\") and not name.lower().startswith(\"__macosx/\")\n",
        "        ]\n",
        "        if len(members) == 0:\n",
        "            raise FileNotFoundError(f\"No data files found in zip: {zip_path}\")\n",
        "\n",
        "        preferred_exts = (\".csv\", \".arff\", \".data\", \".txt\", \".xlsx\", \".xls\")\n",
        "        ordered_members = []\n",
        "        for ext in preferred_exts:\n",
        "            ordered_members.extend([name for name in members if name.lower().endswith(ext)])\n",
        "        ordered_members.extend([name for name in members if name not in ordered_members])\n",
        "\n",
        "        last_error = None\n",
        "        for selected in ordered_members:\n",
        "            try:\n",
        "                with zf.open(selected) as f:\n",
        "                    lower = selected.lower()\n",
        "                    if lower.endswith(\".csv\"):\n",
        "                        df = pd.read_csv(f)\n",
        "                    elif lower.endswith(\".arff\"):\n",
        "                        with TextIOWrapper(f, encoding=\"utf-8\", errors=\"ignore\") as txt_f:\n",
        "                            data, _ = arff.loadarff(txt_f)\n",
        "                        df = pd.DataFrame(data)\n",
        "                        for col in df.columns:\n",
        "                            if df[col].dtype == object:\n",
        "                                df[col] = df[col].apply(\n",
        "                                    lambda value: value.decode(\"utf-8\") if isinstance(value, (bytes, bytearray)) else value\n",
        "                                )\n",
        "                    elif lower.endswith(\".xlsx\") or lower.endswith(\".xls\"):\n",
        "                        df = pd.read_excel(BytesIO(f.read()))\n",
        "                    else:\n",
        "                        df = pd.read_csv(f)\n",
        "                return _strip_object_columns(df)\n",
        "            except Exception as exc:\n",
        "                last_error = exc\n",
        "\n",
        "        raise RuntimeError(f\"Failed to parse any file from zip: {zip_path}\") from last_error\n",
        "\n",
        "\n",
        "def get_benchmark_datasets(allow_uci_download: bool = True):\n",
        "    datasets = []\n",
        "    root_missing_drop_threshold = 0.20\n",
        "\n",
        "    # 1) UCI Breast Cancer Wisconsin (Diagnostic)\n",
        "    # - Remove ID column.\n",
        "    # - All features are continuous.\n",
        "    wdbc_feature_names = [\n",
        "        \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n",
        "        \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
        "        \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n",
        "        \"compactness_se\", \"concavity_se\", \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
        "        \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n",
        "        \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\",\n",
        "    ]\n",
        "    wdbc_cols = [\"id\", \"target\"] + wdbc_feature_names\n",
        "    wdbc_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/breast-cancer-wisconsin/wdbc.data\",\n",
        "        subdir=\"wdbc\",\n",
        "        filename=\"wdbc.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    wdbc_df = pd.read_csv(wdbc_path, header=None, names=wdbc_cols)\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_wdbc_continuous\",\n",
        "            \"source\": \"uci:wdbc\",\n",
        "            \"X\": wdbc_df[wdbc_feature_names].copy(),\n",
        "            \"y\": wdbc_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\"force_all_continuous\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 2) UCI Adult\n",
        "    # - Drop rows with any missing values.\n",
        "    # - Treat only Categorical/Binary variables as categorical.\n",
        "    # - Treat Integer variables as continuous.\n",
        "    adult_cols = [\n",
        "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
        "        \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
        "        \"hours_per_week\", \"native_country\", \"target\",\n",
        "    ]\n",
        "    adult_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/adult/adult.data\",\n",
        "        subdir=\"adult\",\n",
        "        filename=\"adult.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    adult_df = pd.read_csv(adult_path, header=None, names=adult_cols, na_values=[\"?\"], skipinitialspace=True)\n",
        "    adult_df = _strip_object_columns(adult_df)\n",
        "    adult_df = adult_df.dropna(axis=0).reset_index(drop=True)\n",
        "\n",
        "    adult_categorical_cols = [\n",
        "        \"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\",\n",
        "        \"race\", \"sex\", \"native_country\",\n",
        "    ]\n",
        "    adult_feature_cols = [col for col in adult_cols if col != \"target\"]\n",
        "    adult_continuous_cols = [col for col in adult_feature_cols if col not in adult_categorical_cols]\n",
        "\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_adult_mixed\",\n",
        "            \"source\": \"uci:adult\",\n",
        "            \"X\": adult_df[adult_feature_cols].copy(),\n",
        "            \"y\": adult_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\n",
        "                \"force_categorical_cols\": adult_categorical_cols,\n",
        "                \"force_continuous_cols\": adult_continuous_cols,\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 3) UCI Spambase - all continuous\n",
        "    spambase_cols = [f\"f{i}\" for i in range(1, 58)] + [\"target\"]\n",
        "    spambase_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/spambase/spambase.data\",\n",
        "        subdir=\"spambase\",\n",
        "        filename=\"spambase.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    spambase_df = pd.read_csv(spambase_path, header=None, names=spambase_cols)\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_spambase_continuous\",\n",
        "            \"source\": \"uci:spambase\",\n",
        "            \"X\": spambase_df[[f\"f{i}\" for i in range(1, 58)]].copy(),\n",
        "            \"y\": spambase_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\"force_all_continuous\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4) UCI Mushroom - all categorical\n",
        "    # - Drop stalk-root if its missing rate is high; then drop remaining missing rows.\n",
        "    mushroom_feature_cols = [\n",
        "        \"cap_shape\", \"cap_surface\", \"cap_color\", \"bruises\", \"odor\",\n",
        "        \"gill_attachment\", \"gill_spacing\", \"gill_size\", \"gill_color\", \"stalk_shape\",\n",
        "        \"stalk_root\", \"stalk_surface_above_ring\", \"stalk_surface_below_ring\",\n",
        "        \"stalk_color_above_ring\", \"stalk_color_below_ring\", \"veil_type\", \"veil_color\",\n",
        "        \"ring_number\", \"ring_type\", \"spore_print_color\", \"population\", \"habitat\",\n",
        "    ]\n",
        "    mushroom_cols = [\"target\"] + mushroom_feature_cols\n",
        "    mushroom_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/mushroom/agaricus-lepiota.data\",\n",
        "        subdir=\"mushroom\",\n",
        "        filename=\"agaricus-lepiota.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    mushroom_df = pd.read_csv(mushroom_path, header=None, names=mushroom_cols, na_values=[\"?\"], skipinitialspace=True)\n",
        "    mushroom_df = _strip_object_columns(mushroom_df)\n",
        "\n",
        "    stalk_root_missing_rate = float(mushroom_df[\"stalk_root\"].isna().mean())\n",
        "    if stalk_root_missing_rate > root_missing_drop_threshold:\n",
        "        mushroom_df = mushroom_df.drop(columns=[\"stalk_root\"])\n",
        "\n",
        "    mushroom_df = mushroom_df.dropna(axis=0).reset_index(drop=True)\n",
        "    mushroom_X_cols = [col for col in mushroom_df.columns if col != \"target\"]\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_mushroom_categorical\",\n",
        "            \"source\": \"uci:mushroom\",\n",
        "            \"X\": mushroom_df[mushroom_X_cols].copy(),\n",
        "            \"y\": mushroom_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\"force_all_categorical\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 5) UCI MAGIC Gamma Telescope - all continuous\n",
        "    magic_feature_cols = [\n",
        "        \"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\",\n",
        "        \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\",\n",
        "    ]\n",
        "    magic_cols = magic_feature_cols + [\"target\"]\n",
        "    magic_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/magic/magic04.data\",\n",
        "        subdir=\"magic_gamma_telescope\",\n",
        "        filename=\"magic04.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    magic_df = pd.read_csv(magic_path, header=None, names=magic_cols)\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_magic_gamma_continuous\",\n",
        "            \"source\": \"uci:magic-gamma-telescope\",\n",
        "            \"X\": magic_df[magic_feature_cols].copy(),\n",
        "            \"y\": magic_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\"force_all_continuous\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 6) UCI Car Evaluation - all categorical\n",
        "    # Target mapping: {unacc, acc} vs {good, vgood}\n",
        "    car_cols = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"target\"]\n",
        "    car_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/car/car.data\",\n",
        "        subdir=\"car_evaluation\",\n",
        "        filename=\"car.data\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    car_df = pd.read_csv(car_path, header=None, names=car_cols)\n",
        "    car_df = _strip_object_columns(car_df)\n",
        "\n",
        "    car_binary_target = car_df[\"target\"].map(\n",
        "        {\n",
        "            \"unacc\": \"unacc_or_acc\",\n",
        "            \"acc\": \"unacc_or_acc\",\n",
        "            \"good\": \"good_or_vgood\",\n",
        "            \"vgood\": \"good_or_vgood\",\n",
        "        }\n",
        "    )\n",
        "    car_valid = car_binary_target.notna()\n",
        "    car_df = car_df.loc[car_valid].reset_index(drop=True)\n",
        "    car_binary_target = car_binary_target.loc[car_valid].reset_index(drop=True)\n",
        "\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_car_evaluation_categorical\",\n",
        "            \"source\": \"uci:car-evaluation\",\n",
        "            \"X\": car_df[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]].copy(),\n",
        "            \"y\": car_binary_target,\n",
        "            \"schema_hint\": {\"force_all_categorical\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 7) UCI Banknote Authentication - all continuous\n",
        "    banknote_cols = [\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"target\"]\n",
        "    banknote_path = _download_with_cache(\n",
        "        f\"{UCI_BASE_URL}/data_banknote_authentication/data_banknote_authentication.txt\",\n",
        "        subdir=\"banknote_authentication\",\n",
        "        filename=\"data_banknote_authentication.txt\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    banknote_df = pd.read_csv(banknote_path, header=None, names=banknote_cols)\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_banknote_authentication_continuous\",\n",
        "            \"source\": \"uci:banknote-authentication\",\n",
        "            \"X\": banknote_df[[\"variance\", \"skewness\", \"curtosis\", \"entropy\"]].copy(),\n",
        "            \"y\": banknote_df[\"target\"].copy(),\n",
        "            \"schema_hint\": {\"force_all_continuous\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 8) UCI Rice (Cammeo and Osmancik) - all continuous (including integer columns)\n",
        "    rice_zip_path = _download_with_cache(\n",
        "        \"https://archive.ics.uci.edu/static/public/545/rice+cammeo+and+osmancik.zip\",\n",
        "        subdir=\"rice_cammeo_osmancik\",\n",
        "        filename=\"rice+cammeo+and+osmancik.zip\",\n",
        "        allow_download=allow_uci_download,\n",
        "    )\n",
        "    rice_df = _read_uci_table_from_zip(rice_zip_path)\n",
        "    rice_target_col = next(\n",
        "        (candidate for candidate in [\"Class\", \"class\", \"target\", \"Target\"] if candidate in rice_df.columns),\n",
        "        rice_df.columns[-1],\n",
        "    )\n",
        "    rice_feature_cols = [col for col in rice_df.columns if col != rice_target_col]\n",
        "    datasets.append(\n",
        "        {\n",
        "            \"name\": \"uci_rice_cammeo_osmancik_continuous\",\n",
        "            \"source\": \"uci:rice-cammeo-and-osmancik\",\n",
        "            \"X\": rice_df[rice_feature_cols].copy(),\n",
        "            \"y\": rice_df[rice_target_col].copy(),\n",
        "            \"schema_hint\": {\"force_all_continuous\": True},\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "benchmark_datasets = get_benchmark_datasets(allow_uci_download=ALLOW_UCI_DOWNLOAD)\n",
        "pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"dataset\": d[\"name\"],\n",
        "            \"source\": d[\"source\"],\n",
        "            \"rows\": int(d[\"X\"].shape[0]),\n",
        "            \"features\": int(d[\"X\"].shape[1]),\n",
        "        }\n",
        "        for d in benchmark_datasets\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "70106eef",
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_feature_schema(df: pd.DataFrame, schema_hint: Optional[Dict] = None) -> Dict[str, str]:\n",
        "    schema_hint = schema_hint or {}\n",
        "    force_all_categorical = bool(schema_hint.get(\"force_all_categorical\", False))\n",
        "    force_all_continuous = bool(schema_hint.get(\"force_all_continuous\", False))\n",
        "    force_categorical_cols = set(schema_hint.get(\"force_categorical_cols\", []))\n",
        "    force_continuous_cols = set(schema_hint.get(\"force_continuous_cols\", []))\n",
        "\n",
        "    if force_all_categorical and force_all_continuous:\n",
        "        raise ValueError(\"Cannot force all features to both categorical and continuous.\")\n",
        "    overlap = force_categorical_cols & force_continuous_cols\n",
        "    if len(overlap) > 0:\n",
        "        raise ValueError(f\"Columns listed as both categorical and continuous: {sorted(overlap)}\")\n",
        "\n",
        "    schema: Dict[str, str] = {}\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        if force_all_categorical:\n",
        "            schema[col] = \"categorical\"\n",
        "            continue\n",
        "        if force_all_continuous:\n",
        "            schema[col] = \"continuous\"\n",
        "            continue\n",
        "        if col in force_continuous_cols:\n",
        "            schema[col] = \"continuous\"\n",
        "            continue\n",
        "        if col in force_categorical_cols:\n",
        "            schema[col] = \"categorical\"\n",
        "            continue\n",
        "\n",
        "        if (\n",
        "            pd.api.types.is_object_dtype(s)\n",
        "            or isinstance(getattr(s, \"dtype\", None), pd.CategoricalDtype)\n",
        "            or pd.api.types.is_bool_dtype(s)\n",
        "        ):\n",
        "            schema[col] = \"categorical\"\n",
        "        elif pd.api.types.is_integer_dtype(s) and int(s.nunique(dropna=True)) <= 20:\n",
        "            schema[col] = \"categorical\"\n",
        "        else:\n",
        "            schema[col] = \"continuous\"\n",
        "    return schema\n",
        "\n",
        "def encode_dataset_with_schema(\n",
        "    df: pd.DataFrame,\n",
        "    schema: Dict[str, str],\n",
        "    max_categorical_classes: int,\n",
        "):\n",
        "    encoded = pd.DataFrame(index=df.index)\n",
        "    metadata_rows = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        kind = schema[col]\n",
        "        s = df[col]\n",
        "        raw_unique = int(pd.Series(s).nunique(dropna=True))\n",
        "\n",
        "        if kind == \"categorical\":\n",
        "            s_obj = pd.Series(s, copy=False)\n",
        "            s_obj = s_obj.where(s_obj.notna(), \"__MISSING__\").astype(\"string\")\n",
        "\n",
        "            counts = s_obj.value_counts(dropna=False)\n",
        "            if counts.shape[0] > max_categorical_classes:\n",
        "                keep_n = max(1, max_categorical_classes - 1)\n",
        "                keep_values = set(counts.index[:keep_n].tolist())\n",
        "                s_obj = s_obj.where(s_obj.isin(keep_values), \"__OTHER__\")\n",
        "\n",
        "            cat = pd.Categorical(s_obj.astype(str))\n",
        "            codes = cat.codes.astype(np.int64)\n",
        "            cardinality = int(len(cat.categories))\n",
        "\n",
        "            encoded[col] = codes.astype(np.float32)\n",
        "            metadata_rows.append(\n",
        "                {\n",
        "                    \"feature\": col,\n",
        "                    \"feature_type\": \"categorical\",\n",
        "                    \"raw_unique_values\": raw_unique,\n",
        "                    \"cardinality\": cardinality,\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            fill_value = float(s_num.median()) if s_num.notna().any() else 0.0\n",
        "            s_num = s_num.fillna(fill_value).astype(np.float32)\n",
        "\n",
        "            encoded[col] = s_num\n",
        "            metadata_rows.append(\n",
        "                {\n",
        "                    \"feature\": col,\n",
        "                    \"feature_type\": \"continuous\",\n",
        "                    \"raw_unique_values\": raw_unique,\n",
        "                    \"cardinality\": 1,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    feature_metadata = pd.DataFrame(metadata_rows)\n",
        "    feature_is_categorical = (feature_metadata[\"feature_type\"].to_numpy() == \"categorical\")\n",
        "    feature_cardinalities = feature_metadata[\"cardinality\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "    X_np = encoded.to_numpy(dtype=np.float32)\n",
        "    return X_np, feature_is_categorical, feature_cardinalities, feature_metadata\n",
        "\n",
        "\n",
        "def drop_high_cardinality_categorical_features(\n",
        "    df: pd.DataFrame,\n",
        "    schema: Dict[str, str],\n",
        "    max_allowed_cardinality: int = 10,\n",
        "):\n",
        "    drop_cols = []\n",
        "    for col in df.columns:\n",
        "        if schema.get(col) != \"categorical\":\n",
        "            continue\n",
        "        n_unique = int(pd.Series(df[col]).nunique(dropna=True))\n",
        "        if n_unique > max_allowed_cardinality:\n",
        "            drop_cols.append(col)\n",
        "\n",
        "    if len(drop_cols) > 0:\n",
        "        df = df.drop(columns=drop_cols)\n",
        "\n",
        "    schema = {col: schema[col] for col in df.columns if col in schema}\n",
        "    return df, schema, drop_cols\n",
        "\n",
        "\n",
        "def prepare_dataset(record: Dict, max_categorical_classes: int):\n",
        "    X_raw = record[\"X\"].reset_index(drop=True)\n",
        "    y_raw = pd.Series(record[\"y\"]).reset_index(drop=True)\n",
        "\n",
        "    valid = y_raw.notna()\n",
        "    X_raw = X_raw.loc[valid].reset_index(drop=True)\n",
        "    y_raw = y_raw.loc[valid].reset_index(drop=True)\n",
        "\n",
        "    if y_raw.nunique(dropna=True) != 2:\n",
        "        raise ValueError(f\"Dataset '{record['name']}' is not binary after cleaning.\")\n",
        "\n",
        "    schema = infer_feature_schema(X_raw, schema_hint=record.get(\"schema_hint\"))\n",
        "    X_filtered, schema, dropped_cols = drop_high_cardinality_categorical_features(\n",
        "        X_raw,\n",
        "        schema,\n",
        "        max_allowed_cardinality=10,\n",
        "    )\n",
        "    if X_filtered.shape[1] == 0:\n",
        "        raise ValueError(\n",
        "            f\"Dataset '{record['name']}' has no features after removing categorical columns with >10 classes.\"\n",
        "        )\n",
        "\n",
        "    X_np, feature_is_cat, feature_card, feature_meta = encode_dataset_with_schema(\n",
        "        X_filtered,\n",
        "        schema,\n",
        "        max_categorical_classes=max_categorical_classes,\n",
        "    )\n",
        "    if len(dropped_cols) > 0:\n",
        "        print(f\"[info] {record['name']}: dropped high-cardinality categorical columns: {sorted(dropped_cols)}\")\n",
        "\n",
        "    return {\n",
        "        \"name\": record[\"name\"],\n",
        "        \"source\": record[\"source\"],\n",
        "        \"X\": X_np,\n",
        "        \"y\": y_raw.to_numpy(),\n",
        "        \"feature_is_categorical\": feature_is_cat,\n",
        "        \"feature_cardinalities\": feature_card,\n",
        "        \"feature_metadata\": feature_meta,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "173ade27",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pu_task(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    rng: np.random.Generator,\n",
        "    max_positive_size: int,\n",
        "    unlabeled_labeled_positive_ratio: tuple[int, int],\n",
        "    outlier_rate: float,\n",
        "):\n",
        "    labels = np.unique(y)\n",
        "    if labels.shape[0] != 2:\n",
        "        return None\n",
        "\n",
        "    positive_label = rng.choice(labels)\n",
        "    pos_idx = np.where(y == positive_label)[0]\n",
        "    neg_idx = np.where(y != positive_label)[0]\n",
        "\n",
        "    if len(pos_idx) < 2 or len(neg_idx) < 1:\n",
        "        return None\n",
        "\n",
        "    selected_pos_n = int(min(max_positive_size, len(pos_idx)))\n",
        "    selected_pos_idx = rng.choice(pos_idx, size=selected_pos_n, replace=False)\n",
        "\n",
        "    u_ratio, l_ratio = unlabeled_labeled_positive_ratio\n",
        "    if u_ratio < 0 or l_ratio <= 0 or (u_ratio + l_ratio) <= 0:\n",
        "        raise ValueError(\"UNLABELED_LABELED_POSITIVE_RATIO must be (u, l) with u>=0, l>0.\")\n",
        "\n",
        "    unlabeled_pos_n = int(round(selected_pos_n * (float(u_ratio) / float(u_ratio + l_ratio))))\n",
        "    unlabeled_pos_n = int(np.clip(unlabeled_pos_n, 1, selected_pos_n - 1))\n",
        "    labeled_pos_n = selected_pos_n - unlabeled_pos_n\n",
        "    if labeled_pos_n <= 0 or unlabeled_pos_n <= 0:\n",
        "        return None\n",
        "\n",
        "    labeled_pos_idx = rng.choice(selected_pos_idx, size=labeled_pos_n, replace=False)\n",
        "    unlabeled_pos_idx = np.setdiff1d(selected_pos_idx, labeled_pos_idx, assume_unique=False)\n",
        "\n",
        "    neg_needed = int(round(unlabeled_pos_n * outlier_rate / max(1e-8, 1.0 - outlier_rate)))\n",
        "    if outlier_rate > 0.0 and unlabeled_pos_n > 0:\n",
        "        neg_needed = max(1, neg_needed)\n",
        "    neg_needed = min(neg_needed, len(neg_idx))\n",
        "    if neg_needed <= 0:\n",
        "        return None\n",
        "\n",
        "    unlabeled_neg_idx = rng.choice(neg_idx, size=neg_needed, replace=False)\n",
        "\n",
        "    unlabeled_idx = np.concatenate([unlabeled_pos_idx, unlabeled_neg_idx])\n",
        "    unlabeled_y = np.concatenate(\n",
        "        [\n",
        "            np.zeros(unlabeled_pos_idx.shape[0], dtype=np.int64),  # inlier\n",
        "            np.ones(unlabeled_neg_idx.shape[0], dtype=np.int64),   # outlier\n",
        "        ]\n",
        "    )\n",
        "    perm = rng.permutation(unlabeled_idx.shape[0])\n",
        "    unlabeled_idx = unlabeled_idx[perm]\n",
        "    unlabeled_y = unlabeled_y[perm]\n",
        "\n",
        "    labeled_perm = rng.permutation(labeled_pos_idx.shape[0])\n",
        "    labeled_pos_idx = labeled_pos_idx[labeled_perm]\n",
        "\n",
        "    X_task = np.concatenate([X[labeled_pos_idx], X[unlabeled_idx]], axis=0).astype(np.float32)\n",
        "    y_train = np.zeros(labeled_pos_idx.shape[0], dtype=np.float32)\n",
        "\n",
        "    return {\n",
        "        \"X\": X_task,\n",
        "        \"y_train\": y_train,\n",
        "        \"y_test\": unlabeled_y,\n",
        "        \"train_size\": int(labeled_pos_idx.shape[0]),\n",
        "        \"num_unlabeled_inliers\": int(unlabeled_pos_idx.shape[0]),\n",
        "        \"num_unlabeled_outliers\": int(unlabeled_neg_idx.shape[0]),\n",
        "        \"positive_label\": str(positive_label),\n",
        "    }\n",
        "\n",
        "\n",
        "def fpr_at_fixed_tpr(y_true: np.ndarray, outlier_score: np.ndarray, target_tpr: float = 0.95) -> float:\n",
        "    if np.unique(y_true).shape[0] < 2:\n",
        "        return float(\"nan\")\n",
        "    fpr, tpr, _ = roc_curve(y_true, outlier_score)\n",
        "    valid = np.where(tpr >= target_tpr)[0]\n",
        "    if valid.size == 0:\n",
        "        return 1.0\n",
        "    return float(np.min(fpr[valid]))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_single_pu_task(\n",
        "    model,\n",
        "    task: Dict,\n",
        "    feature_is_categorical: np.ndarray,\n",
        "    feature_cardinalities: np.ndarray,\n",
        "    device: str,\n",
        "    supports_categorical: bool,\n",
        ") -> Dict[str, float]:\n",
        "    x = torch.from_numpy(task[\"X\"]).unsqueeze(0).to(device=device, dtype=torch.float32)\n",
        "    y_train = torch.from_numpy(task[\"y_train\"]).unsqueeze(0).to(device=device, dtype=torch.float32)\n",
        "\n",
        "    if supports_categorical:\n",
        "        feature_is_cat_t = torch.from_numpy(feature_is_categorical).unsqueeze(0).to(device=device, dtype=torch.bool)\n",
        "        feature_card_t = torch.from_numpy(feature_cardinalities).unsqueeze(0).to(device=device, dtype=torch.long)\n",
        "        logits = model(\n",
        "            (x, y_train),\n",
        "            train_test_split_index=task[\"train_size\"],\n",
        "            feature_is_categorical=feature_is_cat_t,\n",
        "            feature_cardinalities=feature_card_t,\n",
        "        ).squeeze(0)\n",
        "    else:\n",
        "        logits = model(\n",
        "            (x, y_train),\n",
        "            train_test_split_index=task[\"train_size\"],\n",
        "        ).squeeze(0)\n",
        "\n",
        "    logits_np = logits.detach().cpu().numpy()\n",
        "    y_true = task[\"y_test\"].astype(np.int64)\n",
        "    y_pred = np.argmax(logits_np, axis=1)\n",
        "\n",
        "    outlier_score = logits_np[:, 1]\n",
        "\n",
        "    binary_ready = np.unique(y_true).shape[0] == 2\n",
        "    outlier_mean = float(np.mean(outlier_score[y_true == 1])) if np.any(y_true == 1) else float(\"nan\")\n",
        "    inlier_mean = float(np.mean(outlier_score[y_true == 0])) if np.any(y_true == 0) else float(\"nan\")\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"balanced_accuracy\": float(balanced_accuracy_score(y_true, y_pred)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, outlier_score)) if binary_ready else float(\"nan\"),\n",
        "        \"average_precision\": float(average_precision_score(y_true, outlier_score)) if binary_ready else float(\"nan\"),\n",
        "        \"fpr_at_tpr_0_80\": float(fpr_at_fixed_tpr(y_true, outlier_score, target_tpr=0.80)),\n",
        "        \"fpr_at_tpr_0_90\": float(fpr_at_fixed_tpr(y_true, outlier_score, target_tpr=0.90)),\n",
        "        \"fpr_at_tpr_0_95\": float(fpr_at_fixed_tpr(y_true, outlier_score, target_tpr=0.95)),\n",
        "        \"outlier_score_gap\": float(outlier_mean - inlier_mean)\n",
        "        if (not np.isnan(outlier_mean) and not np.isnan(inlier_mean))\n",
        "        else float(\"nan\"),\n",
        "    }\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "290a6916",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] uci_adult_mixed: dropped high-cardinality categorical columns: ['education', 'native_country', 'occupation']\n",
            "[info] uci_mushroom_categorical: dropped high-cardinality categorical columns: ['gill_color']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>source</th>\n",
              "      <th>rows</th>\n",
              "      <th>features</th>\n",
              "      <th>continuous_features</th>\n",
              "      <th>categorical_features</th>\n",
              "      <th>max_categorical_cardinality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uci_wdbc_continuous</td>\n",
              "      <td>uci:wdbc</td>\n",
              "      <td>569</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uci_adult_mixed</td>\n",
              "      <td>uci:adult</td>\n",
              "      <td>30162</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uci_spambase_continuous</td>\n",
              "      <td>uci:spambase</td>\n",
              "      <td>4601</td>\n",
              "      <td>57</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uci_mushroom_categorical</td>\n",
              "      <td>uci:mushroom</td>\n",
              "      <td>8124</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uci_magic_gamma_continuous</td>\n",
              "      <td>uci:magic-gamma-telescope</td>\n",
              "      <td>19020</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>uci_car_evaluation_categorical</td>\n",
              "      <td>uci:car-evaluation</td>\n",
              "      <td>1728</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>uci_banknote_authentication_continuous</td>\n",
              "      <td>uci:banknote-authentication</td>\n",
              "      <td>1372</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>uci_rice_cammeo_osmancik_continuous</td>\n",
              "      <td>uci:rice-cammeo-and-osmancik</td>\n",
              "      <td>3810</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  dataset                        source  \\\n",
              "0                     uci_wdbc_continuous                      uci:wdbc   \n",
              "1                         uci_adult_mixed                     uci:adult   \n",
              "2                 uci_spambase_continuous                  uci:spambase   \n",
              "3                uci_mushroom_categorical                  uci:mushroom   \n",
              "4              uci_magic_gamma_continuous     uci:magic-gamma-telescope   \n",
              "5          uci_car_evaluation_categorical            uci:car-evaluation   \n",
              "6  uci_banknote_authentication_continuous   uci:banknote-authentication   \n",
              "7     uci_rice_cammeo_osmancik_continuous  uci:rice-cammeo-and-osmancik   \n",
              "\n",
              "    rows  features  continuous_features  categorical_features  \\\n",
              "0    569        30                   30                     0   \n",
              "1  30162        11                    6                     5   \n",
              "2   4601        57                   57                     0   \n",
              "3   8124        20                    0                    20   \n",
              "4  19020        10                   10                     0   \n",
              "5   1728         6                    0                     6   \n",
              "6   1372         4                    4                     0   \n",
              "7   3810         7                    7                     0   \n",
              "\n",
              "   max_categorical_cardinality  \n",
              "0                            1  \n",
              "1                            7  \n",
              "2                            1  \n",
              "3                           10  \n",
              "4                            1  \n",
              "5                            4  \n",
              "6                            1  \n",
              "7                            1  "
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepared_datasets = [prepare_dataset(d, max_categorical_classes=MAX_CATEGORICAL_CLASSES) for d in benchmark_datasets]\n",
        "\n",
        "profile_rows = []\n",
        "for d in prepared_datasets:\n",
        "    feature_meta = d[\"feature_metadata\"]\n",
        "    num_cat = int((feature_meta[\"feature_type\"] == \"categorical\").sum())\n",
        "    num_cont = int((feature_meta[\"feature_type\"] == \"continuous\").sum())\n",
        "    max_card = int(feature_meta.loc[feature_meta[\"feature_type\"] == \"categorical\", \"cardinality\"].max()) if num_cat > 0 else 1\n",
        "    profile_rows.append(\n",
        "        {\n",
        "            \"dataset\": d[\"name\"],\n",
        "            \"source\": d[\"source\"],\n",
        "            \"rows\": int(d[\"X\"].shape[0]),\n",
        "            \"features\": int(d[\"X\"].shape[1]),\n",
        "            \"continuous_features\": num_cont,\n",
        "            \"categorical_features\": num_cat,\n",
        "            \"max_categorical_cardinality\": max_card,\n",
        "        }\n",
        "    )\n",
        "\n",
        "profile_df = pd.DataFrame(profile_rows)\n",
        "profile_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "dc70078f",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_master = np.random.default_rng(GLOBAL_SEED)\n",
        "\n",
        "replicate_frames = []\n",
        "\n",
        "for dataset_idx, dataset in enumerate(prepared_datasets):\n",
        "    ds_seed = int(rng_master.integers(0, 2**31 - 1))\n",
        "    ds_rng = np.random.default_rng(ds_seed)\n",
        "\n",
        "    rows = []\n",
        "    attempts = 0\n",
        "    collected_replicates = 0\n",
        "\n",
        "    while collected_replicates < N_REPLICATES and attempts < MAX_ATTEMPTS_PER_DATASET:\n",
        "        attempts += 1\n",
        "        task = build_pu_task(\n",
        "            X=dataset[\"X\"],\n",
        "            y=dataset[\"y\"],\n",
        "            rng=ds_rng,\n",
        "            max_positive_size=MAX_POSITIVE_SIZE,\n",
        "            unlabeled_labeled_positive_ratio=UNLABELED_LABELED_POSITIVE_RATIO,\n",
        "            outlier_rate=OUTLIER_RATE,\n",
        "        )\n",
        "        if task is None:\n",
        "            continue\n",
        "\n",
        "        unlabeled_total = int(task[\"num_unlabeled_inliers\"] + task[\"num_unlabeled_outliers\"])\n",
        "        real_outlier_proportion = (\n",
        "            float(task[\"num_unlabeled_outliers\"]) / float(unlabeled_total) if unlabeled_total > 0 else float(\"nan\")\n",
        "        )\n",
        "        real_unlabeled_positive_to_labeled_positive_ratio = (\n",
        "            float(task[\"num_unlabeled_inliers\"]) / float(task[\"train_size\"]) if int(task[\"train_size\"]) > 0 else float(\"nan\")\n",
        "        )\n",
        "        real_positive_only_sample_size = int(task[\"train_size\"] + task[\"num_unlabeled_inliers\"])\n",
        "\n",
        "        for model_spec in MODEL_SPECS:\n",
        "            metric = evaluate_single_pu_task(\n",
        "                model=model_spec[\"model\"],\n",
        "                task=task,\n",
        "                feature_is_categorical=dataset[\"feature_is_categorical\"],\n",
        "                feature_cardinalities=dataset[\"feature_cardinalities\"],\n",
        "                device=DEVICE,\n",
        "                supports_categorical=bool(model_spec[\"supports_categorical\"]),\n",
        "            )\n",
        "            metric.update(\n",
        "                {\n",
        "                    \"model_name\": model_spec[\"model_name\"],\n",
        "                    \"dataset\": dataset[\"name\"],\n",
        "                    \"source\": dataset[\"source\"],\n",
        "                    \"replicate\": collected_replicates + 1,\n",
        "                    \"attempt\": attempts,\n",
        "                    \"positive_label\": task[\"positive_label\"],\n",
        "                    \"labeled_positive_size\": int(task[\"train_size\"]),\n",
        "                    \"real_labeled_positive_size\": int(task[\"train_size\"]),\n",
        "                    \"real_positive_only_sample_size\": real_positive_only_sample_size,\n",
        "                    \"unlabeled_inlier_size\": int(task[\"num_unlabeled_inliers\"]),\n",
        "                    \"unlabeled_outlier_size\": int(task[\"num_unlabeled_outliers\"]),\n",
        "                    \"real_outlier_proportion\": real_outlier_proportion,\n",
        "                    \"real_unlabeled_positive_to_labeled_positive_ratio\": real_unlabeled_positive_to_labeled_positive_ratio,\n",
        "                }\n",
        "            )\n",
        "            rows.append(metric)\n",
        "\n",
        "        collected_replicates += 1\n",
        "\n",
        "    if collected_replicates < N_REPLICATES:\n",
        "        print(\n",
        "            f\"[warn] dataset={dataset['name']} collected {collected_replicates} replicates \"\n",
        "            f\"within {MAX_ATTEMPTS_PER_DATASET} attempts.\"\n",
        "        )\n",
        "\n",
        "    rep_df = pd.DataFrame(rows)\n",
        "    replicate_frames.append(rep_df)\n",
        "\n",
        "replicate_results_df = pd.concat(replicate_frames, ignore_index=True) if len(replicate_frames) > 0 else pd.DataFrame()\n",
        "\n",
        "metric_columns = [\n",
        "    \"accuracy\",\n",
        "    \"balanced_accuracy\",\n",
        "    \"roc_auc\",\n",
        "    \"average_precision\",\n",
        "    \"fpr_at_tpr_0_80\",\n",
        "    \"fpr_at_tpr_0_90\",\n",
        "    \"fpr_at_tpr_0_95\",\n",
        "    \"outlier_score_gap\",\n",
        "]\n",
        "\n",
        "if replicate_results_df.empty:\n",
        "    metrics_by_model_df = pd.DataFrame(\n",
        "        columns=[\"dataset\", \"source\", \"model_name\", \"replicates\"] + metric_columns\n",
        "    )\n",
        "    metrics_latest_df = pd.DataFrame(columns=[\"dataset\", \"source\", \"replicates\"] + metric_columns)\n",
        "    metrics_legacy_df = pd.DataFrame(columns=[\"dataset\", \"source\", \"replicates\"] + metric_columns)\n",
        "\n",
        "    metrics_summary_df = pd.DataFrame(\n",
        "        columns=[\"dataset\", \"source\", \"replicates_latest\", \"replicates_legacy\"]\n",
        "        + [\n",
        "            col_name\n",
        "            for metric in metric_columns\n",
        "            for col_name in (\n",
        "                f\"{metric}_latest\",\n",
        "                f\"{metric}_legacy\",\n",
        "                f\"{metric}_delta_latest_minus_legacy\",\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    composition_summary_df = pd.DataFrame(\n",
        "        columns=[\n",
        "            \"dataset\",\n",
        "            \"source\",\n",
        "            \"replicates\",\n",
        "            \"true_positive_only_sample_size\",\n",
        "            \"true_unlabeled_to_labeled_positive_ratio\",\n",
        "            \"true_outlier_rate\",\n",
        "        ]\n",
        "    )\n",
        "else:\n",
        "    metrics_by_model_df = (\n",
        "        replicate_results_df.groupby([\"dataset\", \"source\", \"model_name\"], as_index=False)\n",
        "        .agg(\n",
        "            replicates=(\"replicate\", \"count\"),\n",
        "            accuracy=(\"accuracy\", \"mean\"),\n",
        "            balanced_accuracy=(\"balanced_accuracy\", \"mean\"),\n",
        "            roc_auc=(\"roc_auc\", \"mean\"),\n",
        "            average_precision=(\"average_precision\", \"mean\"),\n",
        "            fpr_at_tpr_0_80=(\"fpr_at_tpr_0_80\", \"mean\"),\n",
        "            fpr_at_tpr_0_90=(\"fpr_at_tpr_0_90\", \"mean\"),\n",
        "            fpr_at_tpr_0_95=(\"fpr_at_tpr_0_95\", \"mean\"),\n",
        "            outlier_score_gap=(\"outlier_score_gap\", \"mean\"),\n",
        "        )\n",
        "        .sort_values([\"dataset\", \"model_name\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    metrics_latest_df = (\n",
        "        metrics_by_model_df[metrics_by_model_df[\"model_name\"] == \"latest\"]\n",
        "        .drop(columns=[\"model_name\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    metrics_legacy_df = (\n",
        "        metrics_by_model_df[metrics_by_model_df[\"model_name\"] == \"legacy\"]\n",
        "        .drop(columns=[\"model_name\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    latest_for_merge = metrics_latest_df.rename(\n",
        "        columns={\"replicates\": \"replicates_latest\", **{metric: f\"{metric}_latest\" for metric in metric_columns}}\n",
        "    )\n",
        "    legacy_for_merge = metrics_legacy_df.rename(\n",
        "        columns={\"replicates\": \"replicates_legacy\", **{metric: f\"{metric}_legacy\" for metric in metric_columns}}\n",
        "    )\n",
        "\n",
        "    metrics_summary_df = (\n",
        "        latest_for_merge.merge(legacy_for_merge, on=[\"dataset\", \"source\"], how=\"outer\")\n",
        "        .sort_values(\"dataset\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    for metric in metric_columns:\n",
        "        latest_col = f\"{metric}_latest\"\n",
        "        legacy_col = f\"{metric}_legacy\"\n",
        "        delta_col = f\"{metric}_delta_latest_minus_legacy\"\n",
        "        if latest_col in metrics_summary_df.columns and legacy_col in metrics_summary_df.columns:\n",
        "            metrics_summary_df[delta_col] = metrics_summary_df[latest_col] - metrics_summary_df[legacy_col]\n",
        "        else:\n",
        "            metrics_summary_df[delta_col] = float(\"nan\")\n",
        "\n",
        "    # True PU composition is task-defined (independent of model); compute once from latest rows.\n",
        "    composition_source_df = replicate_results_df[replicate_results_df[\"model_name\"] == \"latest\"]\n",
        "    composition_summary_df = (\n",
        "        composition_source_df.groupby([\"dataset\", \"source\"], as_index=False)\n",
        "        .agg(\n",
        "            replicates=(\"replicate\", \"count\"),\n",
        "            true_positive_only_sample_size=(\"real_positive_only_sample_size\", \"mean\"),\n",
        "            true_unlabeled_to_labeled_positive_ratio=(\"real_unlabeled_positive_to_labeled_positive_ratio\", \"mean\"),\n",
        "            true_outlier_rate=(\"real_outlier_proportion\", \"mean\"),\n",
        "        )\n",
        "        .sort_values(\"dataset\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "6b5b24b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance metrics over replicates: latest model\n",
            "outlier_score_gap = mean(outlier score for true outliers) - mean(outlier score for true inliers)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>source</th>\n",
              "      <th>replicates</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>balanced_accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>average_precision</th>\n",
              "      <th>fpr_at_tpr_0_80</th>\n",
              "      <th>fpr_at_tpr_0_90</th>\n",
              "      <th>fpr_at_tpr_0_95</th>\n",
              "      <th>outlier_score_gap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uci_adult_mixed</td>\n",
              "      <td>uci:adult</td>\n",
              "      <td>10</td>\n",
              "      <td>0.586667</td>\n",
              "      <td>0.586667</td>\n",
              "      <td>0.801154</td>\n",
              "      <td>0.816323</td>\n",
              "      <td>0.392778</td>\n",
              "      <td>0.531111</td>\n",
              "      <td>0.646111</td>\n",
              "      <td>0.749083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uci_banknote_authentication_continuous</td>\n",
              "      <td>uci:banknote-authentication</td>\n",
              "      <td>10</td>\n",
              "      <td>0.997126</td>\n",
              "      <td>0.997126</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.949923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uci_car_evaluation_categorical</td>\n",
              "      <td>uci:car-evaluation</td>\n",
              "      <td>10</td>\n",
              "      <td>0.519615</td>\n",
              "      <td>0.564907</td>\n",
              "      <td>0.968238</td>\n",
              "      <td>0.963689</td>\n",
              "      <td>0.034074</td>\n",
              "      <td>0.115926</td>\n",
              "      <td>0.174815</td>\n",
              "      <td>0.978525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uci_magic_gamma_continuous</td>\n",
              "      <td>uci:magic-gamma-telescope</td>\n",
              "      <td>10</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>0.876778</td>\n",
              "      <td>0.880072</td>\n",
              "      <td>0.222778</td>\n",
              "      <td>0.353333</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>1.230480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uci_mushroom_categorical</td>\n",
              "      <td>uci:mushroom</td>\n",
              "      <td>10</td>\n",
              "      <td>0.897500</td>\n",
              "      <td>0.897500</td>\n",
              "      <td>0.995336</td>\n",
              "      <td>0.996065</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.013889</td>\n",
              "      <td>3.222485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>uci_rice_cammeo_osmancik_continuous</td>\n",
              "      <td>uci:rice-cammeo-and-osmancik</td>\n",
              "      <td>10</td>\n",
              "      <td>0.919444</td>\n",
              "      <td>0.919444</td>\n",
              "      <td>0.979907</td>\n",
              "      <td>0.981574</td>\n",
              "      <td>0.016111</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.098889</td>\n",
              "      <td>5.082655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>uci_spambase_continuous</td>\n",
              "      <td>uci:spambase</td>\n",
              "      <td>10</td>\n",
              "      <td>0.868333</td>\n",
              "      <td>0.868333</td>\n",
              "      <td>0.941954</td>\n",
              "      <td>0.949417</td>\n",
              "      <td>0.068889</td>\n",
              "      <td>0.171667</td>\n",
              "      <td>0.315556</td>\n",
              "      <td>2.488679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>uci_wdbc_continuous</td>\n",
              "      <td>uci:wdbc</td>\n",
              "      <td>10</td>\n",
              "      <td>0.948558</td>\n",
              "      <td>0.948558</td>\n",
              "      <td>0.992365</td>\n",
              "      <td>0.993018</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.018075</td>\n",
              "      <td>0.040275</td>\n",
              "      <td>6.041316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  dataset                        source  \\\n",
              "0                         uci_adult_mixed                     uci:adult   \n",
              "1  uci_banknote_authentication_continuous   uci:banknote-authentication   \n",
              "2          uci_car_evaluation_categorical            uci:car-evaluation   \n",
              "3              uci_magic_gamma_continuous     uci:magic-gamma-telescope   \n",
              "4                uci_mushroom_categorical                  uci:mushroom   \n",
              "5     uci_rice_cammeo_osmancik_continuous  uci:rice-cammeo-and-osmancik   \n",
              "6                 uci_spambase_continuous                  uci:spambase   \n",
              "7                     uci_wdbc_continuous                      uci:wdbc   \n",
              "\n",
              "   replicates  accuracy  balanced_accuracy   roc_auc  average_precision  \\\n",
              "0          10  0.586667           0.586667  0.801154           0.816323   \n",
              "1          10  0.997126           0.997126  1.000000           1.000000   \n",
              "2          10  0.519615           0.564907  0.968238           0.963689   \n",
              "3          10  0.782500           0.782500  0.876778           0.880072   \n",
              "4          10  0.897500           0.897500  0.995336           0.996065   \n",
              "5          10  0.919444           0.919444  0.979907           0.981574   \n",
              "6          10  0.868333           0.868333  0.941954           0.949417   \n",
              "7          10  0.948558           0.948558  0.992365           0.993018   \n",
              "\n",
              "   fpr_at_tpr_0_80  fpr_at_tpr_0_90  fpr_at_tpr_0_95  outlier_score_gap  \n",
              "0         0.392778         0.531111         0.646111           0.749083  \n",
              "1         0.000000         0.000000         0.000000           5.949923  \n",
              "2         0.034074         0.115926         0.174815           0.978525  \n",
              "3         0.222778         0.353333         0.472222           1.230480  \n",
              "4         0.000000         0.000556         0.013889           3.222485  \n",
              "5         0.016111         0.050000         0.098889           5.082655  \n",
              "6         0.068889         0.171667         0.315556           2.488679  \n",
              "7         0.002381         0.018075         0.040275           6.041316  "
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Performance metrics over replicates: latest model\")\n",
        "print(\"outlier_score_gap = mean(outlier score for true outliers) - mean(outlier score for true inliers)\")\n",
        "metrics_latest_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "f9af922f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance metrics over replicates: legacy model\n",
            "outlier_score_gap = mean(outlier score for true outliers) - mean(outlier score for true inliers)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>source</th>\n",
              "      <th>replicates</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>balanced_accuracy</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>average_precision</th>\n",
              "      <th>fpr_at_tpr_0_80</th>\n",
              "      <th>fpr_at_tpr_0_90</th>\n",
              "      <th>fpr_at_tpr_0_95</th>\n",
              "      <th>outlier_score_gap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uci_adult_mixed</td>\n",
              "      <td>uci:adult</td>\n",
              "      <td>10</td>\n",
              "      <td>0.773056</td>\n",
              "      <td>0.773056</td>\n",
              "      <td>0.865173</td>\n",
              "      <td>0.872042</td>\n",
              "      <td>0.236111</td>\n",
              "      <td>0.382222</td>\n",
              "      <td>0.554444</td>\n",
              "      <td>0.808146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uci_banknote_authentication_continuous</td>\n",
              "      <td>uci:banknote-authentication</td>\n",
              "      <td>10</td>\n",
              "      <td>0.997865</td>\n",
              "      <td>0.997865</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.952335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uci_car_evaluation_categorical</td>\n",
              "      <td>uci:car-evaluation</td>\n",
              "      <td>10</td>\n",
              "      <td>0.888382</td>\n",
              "      <td>0.885370</td>\n",
              "      <td>0.965346</td>\n",
              "      <td>0.958349</td>\n",
              "      <td>0.051481</td>\n",
              "      <td>0.097778</td>\n",
              "      <td>0.190185</td>\n",
              "      <td>1.471585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uci_magic_gamma_continuous</td>\n",
              "      <td>uci:magic-gamma-telescope</td>\n",
              "      <td>10</td>\n",
              "      <td>0.779722</td>\n",
              "      <td>0.779722</td>\n",
              "      <td>0.873685</td>\n",
              "      <td>0.875595</td>\n",
              "      <td>0.218889</td>\n",
              "      <td>0.371667</td>\n",
              "      <td>0.517778</td>\n",
              "      <td>1.128709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uci_mushroom_categorical</td>\n",
              "      <td>uci:mushroom</td>\n",
              "      <td>10</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.999056</td>\n",
              "      <td>0.999109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.882901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>uci_rice_cammeo_osmancik_continuous</td>\n",
              "      <td>uci:rice-cammeo-and-osmancik</td>\n",
              "      <td>10</td>\n",
              "      <td>0.914444</td>\n",
              "      <td>0.914444</td>\n",
              "      <td>0.980040</td>\n",
              "      <td>0.981293</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>0.047222</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>4.815327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>uci_spambase_continuous</td>\n",
              "      <td>uci:spambase</td>\n",
              "      <td>10</td>\n",
              "      <td>0.851944</td>\n",
              "      <td>0.851944</td>\n",
              "      <td>0.921537</td>\n",
              "      <td>0.936016</td>\n",
              "      <td>0.107778</td>\n",
              "      <td>0.248333</td>\n",
              "      <td>0.461111</td>\n",
              "      <td>1.779089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>uci_wdbc_continuous</td>\n",
              "      <td>uci:wdbc</td>\n",
              "      <td>10</td>\n",
              "      <td>0.951425</td>\n",
              "      <td>0.951425</td>\n",
              "      <td>0.992657</td>\n",
              "      <td>0.993386</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.014722</td>\n",
              "      <td>0.045372</td>\n",
              "      <td>5.599562</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  dataset                        source  \\\n",
              "0                         uci_adult_mixed                     uci:adult   \n",
              "1  uci_banknote_authentication_continuous   uci:banknote-authentication   \n",
              "2          uci_car_evaluation_categorical            uci:car-evaluation   \n",
              "3              uci_magic_gamma_continuous     uci:magic-gamma-telescope   \n",
              "4                uci_mushroom_categorical                  uci:mushroom   \n",
              "5     uci_rice_cammeo_osmancik_continuous  uci:rice-cammeo-and-osmancik   \n",
              "6                 uci_spambase_continuous                  uci:spambase   \n",
              "7                     uci_wdbc_continuous                      uci:wdbc   \n",
              "\n",
              "   replicates  accuracy  balanced_accuracy   roc_auc  average_precision  \\\n",
              "0          10  0.773056           0.773056  0.865173           0.872042   \n",
              "1          10  0.997865           0.997865  1.000000           1.000000   \n",
              "2          10  0.888382           0.885370  0.965346           0.958349   \n",
              "3          10  0.779722           0.779722  0.873685           0.875595   \n",
              "4          10  0.986111           0.986111  0.999056           0.999109   \n",
              "5          10  0.914444           0.914444  0.980040           0.981293   \n",
              "6          10  0.851944           0.851944  0.921537           0.936016   \n",
              "7          10  0.951425           0.951425  0.992657           0.993386   \n",
              "\n",
              "   fpr_at_tpr_0_80  fpr_at_tpr_0_90  fpr_at_tpr_0_95  outlier_score_gap  \n",
              "0         0.236111         0.382222         0.554444           0.808146  \n",
              "1         0.000000         0.000000         0.000000           5.952335  \n",
              "2         0.051481         0.097778         0.190185           1.471585  \n",
              "3         0.218889         0.371667         0.517778           1.128709  \n",
              "4         0.000000         0.000000         0.000000           3.882901  \n",
              "5         0.016667         0.047222         0.101667           4.815327  \n",
              "6         0.107778         0.248333         0.461111           1.779089  \n",
              "7         0.002381         0.014722         0.045372           5.599562  "
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Performance metrics over replicates: legacy model\")\n",
        "print(\"outlier_score_gap = mean(outlier score for true outliers) - mean(outlier score for true inliers)\")\n",
        "metrics_legacy_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "ffc25b30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True PU composition over replicates (additional table):\n",
            "Saved evaluation outputs to: /Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/slim_pretrain/pretrain_v2/evaluation_outputs/eval_20260228_021005\n",
            "- summary_metrics.csv\n",
            "- summary_metrics_latest.csv\n",
            "- summary_metrics_legacy.csv\n",
            "- metrics_by_model.csv\n",
            "- replicate_metrics.csv\n",
            "- dataset_feature_profile.csv\n",
            "- pu_composition_summary.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"True PU composition over replicates (additional table):\")\n",
        "composition_summary_df\n",
        "\n",
        "run_id = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_dir = OUTPUT_DIR / f\"eval_{run_id}\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "summary_path = run_dir / \"summary_metrics.csv\"\n",
        "summary_latest_path = run_dir / \"summary_metrics_latest.csv\"\n",
        "summary_legacy_path = run_dir / \"summary_metrics_legacy.csv\"\n",
        "metrics_by_model_path = run_dir / \"metrics_by_model.csv\"\n",
        "replicate_path = run_dir / \"replicate_metrics.csv\"\n",
        "profile_path = run_dir / \"dataset_feature_profile.csv\"\n",
        "composition_path = run_dir / \"pu_composition_summary.csv\"\n",
        "config_path = run_dir / \"run_config.json\"\n",
        "\n",
        "metrics_summary_df.to_csv(summary_path, index=False)\n",
        "metrics_latest_df.to_csv(summary_latest_path, index=False)\n",
        "metrics_legacy_df.to_csv(summary_legacy_path, index=False)\n",
        "metrics_by_model_df.to_csv(metrics_by_model_path, index=False)\n",
        "replicate_results_df.to_csv(replicate_path, index=False)\n",
        "profile_df.to_csv(profile_path, index=False)\n",
        "composition_summary_df.to_csv(composition_path, index=False)\n",
        "\n",
        "for dataset in prepared_datasets:\n",
        "    safe_name = \"\".join(ch if ch.isalnum() or ch in {\"_\", \"-\"} else \"_\" for ch in dataset[\"name\"])\n",
        "    dataset[\"feature_metadata\"].to_csv(run_dir / f\"feature_metadata_{safe_name}.csv\", index=False)\n",
        "\n",
        "run_config = {\n",
        "    \"checkpoint_path\": str(CHECKPOINT_PATH),\n",
        "    \"legacy_checkpoint_path\": str(LEGACY_CHECKPOINT_PATH),\n",
        "    \"legacy_model_commit\": LEGACY_MODEL_COMMIT,\n",
        "    \"device\": DEVICE,\n",
        "    \"allow_uci_download\": ALLOW_UCI_DOWNLOAD,\n",
        "    \"n_replicates\": N_REPLICATES,\n",
        "    \"max_attempts_per_dataset\": MAX_ATTEMPTS_PER_DATASET,\n",
        "    \"max_positive_size\": MAX_POSITIVE_SIZE,\n",
        "    \"unlabeled_labeled_positive_ratio\": list(UNLABELED_LABELED_POSITIVE_RATIO),\n",
        "    \"outlier_rate\": OUTLIER_RATE,\n",
        "    \"max_categorical_classes\": MAX_CATEGORICAL_CLASSES,\n",
        "    \"global_seed\": GLOBAL_SEED,\n",
        "}\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(run_config, f, indent=2)\n",
        "\n",
        "print(f\"Saved evaluation outputs to: {run_dir}\")\n",
        "print(f\"- {summary_path.name}\")\n",
        "print(f\"- {summary_latest_path.name}\")\n",
        "print(f\"- {summary_legacy_path.name}\")\n",
        "print(f\"- {metrics_by_model_path.name}\")\n",
        "print(f\"- {replicate_path.name}\")\n",
        "print(f\"- {profile_path.name}\")\n",
        "print(f\"- {composition_path.name}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "icl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
