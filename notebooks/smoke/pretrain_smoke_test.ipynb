{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slim_pretrain Smoke Test\n",
    "\n",
    "This notebook performs a small end-to-end pretraining smoke test for the self-contained `slim_pretrain` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/qltian/Library/CloudStorage/GoogleDrive-qltian2021@gmail.com/Other computers/My Laptop/Documents/Research/ai/prior_gen\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Robust repo root discovery\n",
    "candidate = Path.cwd().resolve()\n",
    "while candidate != candidate.parent and not (candidate / \"slim_pretrain\").exists():\n",
    "    candidate = candidate.parent\n",
    "if not (candidate / \"slim_pretrain\").exists():\n",
    "    raise RuntimeError(\"Could not find repo root containing slim_pretrain/.\")\n",
    "\n",
    "repo_root = candidate\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from slim_pretrain.pretrain.data import VariableBatchSpec\n",
    "from slim_pretrain.pretrain.train import (\n",
    "    DataCurriculumConfig,\n",
    "    ModelConfig,\n",
    "    OptimConfig,\n",
    "    PretrainConfig,\n",
    "    default_base_prior_config,\n",
    "    pretrain_nano_tabpfn_pu,\n",
    ")\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured steps: 6\n",
      "Configured stages: 2\n"
     ]
    }
   ],
   "source": [
    "base_cfg = default_base_prior_config()\n",
    "\n",
    "cfg = PretrainConfig(\n",
    "    model=ModelConfig(\n",
    "        embedding_size=32,\n",
    "        num_attention_heads=4,\n",
    "        mlp_hidden_size=64,\n",
    "        num_layers=2,\n",
    "        num_outputs=2,\n",
    "    ),\n",
    "    optim=OptimConfig(\n",
    "        base_lr=1e-3,\n",
    "        min_lr=1e-4,\n",
    "        weight_decay=0.0,\n",
    "        warmup_steps=5,\n",
    "        beta1=0.9,\n",
    "        beta2=0.95,\n",
    "        grad_clip_norm=1.0,\n",
    "    ),\n",
    "    data=DataCurriculumConfig(\n",
    "        total_stages=2,\n",
    "        steps_per_stage=3,\n",
    "        batch_spec=VariableBatchSpec(\n",
    "            batch_size=4,\n",
    "            seq_len_range=(48, 64),\n",
    "            num_features_range=(4, 10),\n",
    "            train_ratio_range=(0.6, 0.8),\n",
    "            pu_row_policy=\"drop\",\n",
    "        ),\n",
    "    ),\n",
    "    device=\"cpu\",\n",
    "    seed=42,\n",
    "    log_every=1,\n",
    "    max_steps=6,\n",
    ")\n",
    "\n",
    "print(\"Configured steps:\", cfg.total_steps)\n",
    "print(\"Configured stages:\", cfg.data.total_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1/6 stage=1 lr=0.000200 loss=0.6882 loss_ema=0.6882 pu_rate=1.00\n",
      "step=2/6 stage=1 lr=0.000400 loss=0.6882 loss_ema=0.6882 pu_rate=1.00\n",
      "step=3/6 stage=1 lr=0.000600 loss=0.7013 loss_ema=0.6888 pu_rate=1.00\n",
      "step=4/6 stage=2 lr=0.000800 loss=0.6587 loss_ema=0.6873 pu_rate=1.00\n",
      "step=5/6 stage=2 lr=0.001000 loss=0.6723 loss_ema=0.6866 pu_rate=1.00\n",
      "step=6/6 stage=2 lr=0.001000 loss=0.7134 loss_ema=0.6879 pu_rate=1.00\n",
      "Smoke pretraining done in 0.79s with 6 steps.\n",
      "Last step record:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step': 5,\n",
       " 'stage': 2,\n",
       " 'lr': 0.001,\n",
       " 'loss': 0.7133632898330688,\n",
       " 'loss_ema': 0.6879215244057775,\n",
       " 'eval_loss': nan,\n",
       " 'is_causal': 0.0,\n",
       " 'num_layers': 5.5,\n",
       " 'hidden_dim': 12.0,\n",
       " 'pu_keep_probability': 0.0,\n",
       " 'batch_pu_rate': 1.0,\n",
       " 'batch_removed_rows_mean': 17.75}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "result = pretrain_nano_tabpfn_pu(base_cfg=base_cfg, config=cfg)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "history = result[\"history\"]\n",
    "print(f\"Smoke pretraining done in {elapsed:.2f}s with {len(history)} steps.\")\n",
    "print(\"Last step record:\")\n",
    "history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOKE TEST PASSED\n",
      "First step: {'step': 0, 'stage': 1, 'lr': 0.0002, 'loss': 0.6881925463676453, 'loss_ema': 0.6881925463676453, 'eval_loss': nan, 'is_causal': 0.0, 'num_layers': 2.0, 'hidden_dim': 8.0, 'pu_keep_probability': 0.0, 'batch_pu_rate': 1.0, 'batch_removed_rows_mean': 20.5}\n",
      "Last step: {'step': 5, 'stage': 2, 'lr': 0.001, 'loss': 0.7133632898330688, 'loss_ema': 0.6879215244057775, 'eval_loss': nan, 'is_causal': 0.0, 'num_layers': 5.5, 'hidden_dim': 12.0, 'pu_keep_probability': 0.0, 'batch_pu_rate': 1.0, 'batch_removed_rows_mean': 17.75}\n"
     ]
    }
   ],
   "source": [
    "assert len(history) == cfg.total_steps, \"History length mismatch.\"\n",
    "assert all(float(r[\"pu_keep_probability\"]) == 0.0 for r in history), \"PU should be always-on.\"\n",
    "assert all(1 <= int(r[\"stage\"]) <= cfg.data.total_stages for r in history), \"Invalid stage index in history.\"\n",
    "assert all(float(r[\"loss\"]) >= 0.0 for r in history), \"Loss must be non-negative.\"\n",
    "\n",
    "print(\"SMOKE TEST PASSED\")\n",
    "print(\"First step:\", history[0])\n",
    "print(\"Last step:\", history[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
